{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf0344e-2247-46b9-ba20-ff452b8644c5",
   "metadata": {},
   "source": [
    "# Cover Page \n",
    "### Student ID: 210003508\n",
    "### Module Code: GG4257 \n",
    "### Module Title: Urban Analytics: A Toolkit for Sustainable Urban Development\n",
    "### Assignment: Lab Assignment No 2 - Networks, Geodemographics and Spatial Microsimulation.\n",
    "### Degree Programme: Geography \n",
    "### Deadline Date: 02.04.2025\n",
    "\n",
    "In submitting this assignment, I hereby confirm that:\n",
    "\n",
    "I have read the University's statement on Good Academic Practice; that the following work is my own work; and that significant academic debts and borrowings have been properly acknowledged and referenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b50b2-f778-4014-9ac2-2bb4e4c839ce",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This section outlines how to replicate the code and access the required data. All data will be available in a **OneDrive folder**, with the code provided in a dedicated GitHub repository.\n",
    "\n",
    "This report documents the work conducted for **Lab Assignment 2**, covering challenges from **Labs 5, 6 and 7**. Each lab section includes problem descriptions, methods, and results. Code is supplemented with comments and markdown explanations, with screenshots of outputs (e.g., maps and graphs) included to ensure clarity and help replication. To meet GitHub size limits, certain output cells have been cleared from the notebook. All data paths in the code assume files are stored in a folder named \"Data\". \n",
    "\n",
    "#### GitHub Username: Ejarrett\n",
    "#### GitHub Repository: [UA_Lab_Assignment_2](https://github.com/ejarrettt/UA_Lab_Assignment_2)\n",
    "#### [OneDrive Folder](https://1drv.ms/f/c/46775ff05561cd60/EtpKCVLnUXtMupoCXG5G86QB-GfpojgYMR0f4hgJI-Fclg?e=bvGWOb)\n",
    "\n",
    "To replicate this report:\n",
    "1. Clone the GitHub repository.\n",
    "2. Download the datasets from the [OneDrive Folder](https://1drv.ms/f/c/46775ff05561cd60/EtpKCVLnUXtMupoCXG5G86QB-GfpojgYMR0f4hgJI-Fclg?e=bvGWOb)\n",
    "3. Unzip the folder and copy the \"Data\" folder to the same place you have stored the notebook. \n",
    "4. Run this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f8c55-cb26-4059-9518-1b18cb76912c",
   "metadata": {},
   "source": [
    "# Lab No 5: Introduction to Networks (2 Challenges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f84e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies \n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa65102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c164b-da19-40b4-ba0a-a95c2b893b1d",
   "metadata": {},
   "source": [
    "## Challenge 1:\n",
    "\n",
    "It's time for you to apply everything you learned by analyzing a case study of FourSquare social Network. (Foursquare is a location-based online social network. The dataset contains a list of all of the user-to-user links)\n",
    "\n",
    "Datasource: @inproceedings{gao2012exploring,\n",
    "     title={Exploring social-historical ties on location-based social networks},\n",
    "     author={Gao, Huiji and Tang, Jiliang and Liu, Huan},\n",
    "     booktitle={Proceedings of the 6th International AAAI Conference on Weblogs and Social Media},\n",
    "     year={2012}\n",
    "}\n",
    "\n",
    "- **Data**: `FS.csv` (avaliable in Moodle)\n",
    "\n",
    "1. Read the FS network dataset.\n",
    "2. Describe using the basic functions of the graph's size. Explore nodes and edges. Provide how many nodes and edges are present in the network.\n",
    "3. The dataset generates a graph of 639.014 nodes, so it is massive and you won't see anything meaningful if you try to plot it. So you need to create a subset using the **degree centrality** to find out find the top 4 of the most important nodes, and use them to create a subset of the original network. \n",
    "4. Extract the degree centrality values and convert them into a list. Then, plot a histogram to visualize the distribution of node degrees in the original network.\n",
    "5. Create a plot for the subset created.\n",
    "6. Now calculate another relevant measure of the network -- **betweenness centrality**. Plot the betweenness centrality distribution of the subset you created. Tip: Same steps from the previous step, but use `nx.betweenness_centrality()`\n",
    "7. Plot the Matrix, Arc and Circos from the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nxviz as nv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fccd2",
   "metadata": {},
   "source": [
    "### Q1. Read the FS network dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "fs_network = pd.read_csv(\"/Users/elenajarrett/Library/CloudStorage/OneDrive-Personal/Year 4/GG4257/Data2/Data/data5/FS.csv\")\n",
    "\n",
    "# Print column names\n",
    "print(fs_network.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph using correct column names\n",
    "G = nx.from_pandas_edgelist(fs_network, source='source', target='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5b2d9",
   "metadata": {},
   "source": [
    "### Q2. Describe using the basic functions of the graph's size. Explore nodes and edges. Provide how many nodes and edges are present in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d926f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G))# Tells you the size of the graph `T`, \n",
    "print(type(G.nodes())) # Tells you the type of `T.nodes()`\n",
    "print(list(G.edges(data=True))[-1]) # Read the attributes associated with the last element of the edges list. \n",
    "print(list(G.nodes(data=True))[0]) # Read the attributes associated with the first element of the node list.\n",
    "print(type(list(G.edges(data=True))[-1][2])) # Which type; therefore, you can see what you are reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d493ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using len to read the size of the graph\n",
    "print(f\"The size of the graph is: {len(G)}\")\n",
    "# Using len to read the number of edges and nodes\n",
    "print(f\"The number of edges in the graph is: {len(G.edges())}\")\n",
    "print(f\"The number of nodes in the graph is: {len(G.nodes())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf823a",
   "metadata": {},
   "source": [
    "### Q3. The dataset generates a graph of 639.014 nodes, so it is massive and you won't see anything meaningful if you try to plot it. So you need to create a subset using the **degree centrality** to find out find the top 4 of the most important nodes, and use them to create a subset of the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9d633-4c00-4f13-b7d1-4d7b80c13b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_of_selfloops(G)\n",
    "# There are no self-loops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9257651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Degree Centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Get the top 4 nodes with highest degree centrality\n",
    "top_4_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:4]\n",
    "print(\"Top 4 nodes with highest degree centrality:\", top_4_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c5a5d-1a41-4772-888b-a158ff542803",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_G=G.edges([106223, 89302, 76517, 66999], data=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66883f8f-b14b-41dd-b884-664e2e73ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sub = nx.DiGraph()\n",
    "len(G_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6247a55-270e-4976-8990-a78073dff780",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sub.add_edges_from(subset_G) #Adding the list from the subset of nodes.\n",
    "len(G_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806d116",
   "metadata": {},
   "source": [
    "### Q4. Extract the degree centrality values and convert them into a list. Then, plot a histogram to visualize the distribution of node degrees in the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa0cad-32f6-4164-9275-3527c8cc30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values and convert them into a list\n",
    "centrality_list = list(degree_centrality.values())\n",
    "\n",
    "# Print the list of degree centrality values\n",
    "print(centrality_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ed6a2-4fec-44d0-b8b2-ddaf26287c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a histogram of degree centrality distribution\n",
    "plt.hist(centrality_list, bins=10, edgecolor=\"black\")\n",
    "plt.xlabel(\"Degree Centrality\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.title(\"Distribution of Node Degrees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b326c",
   "metadata": {},
   "source": [
    "### Q5. Create a plot for the subset created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77613c29-12d9-4de2-8eef-09b62e22d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "pos = nx.spring_layout(G_sub)\n",
    "nx.draw(G_sub, pos, with_labels=True, node_size=700, node_color='skyblue')\n",
    "plt.title('Subgraph with Top 4 Nodes by Degree Centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe17db",
   "metadata": {},
   "source": [
    "### Q6. Now calculate another relevant measure of the network -- **betweenness centrality**. Plot the betweenness centrality distribution of the subset you created. Tip: Same steps from the previous step, but use `nx.betweenness_centrality()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f886c-39de-487f-8dc8-0e140c09b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty graph called git_network\n",
    "G_network_subset = nx.Graph()\n",
    "G_network_subset.add_edges_from(subset_G) # Adding the edges from git.edges\n",
    "\n",
    "# This line here calculates the betweenness centrality for all the notes in \"git\" and stores them in a dictionary (called betweenness list)\n",
    "betweenness_centrality = nx.betweenness_centrality(G_network_subset)\n",
    "\n",
    "# Extracting degree centrality values as a list\n",
    "betweenness_list = list(betweenness_centrality.values())\n",
    "\n",
    "# Print the degree centrality list\n",
    "#print(betweenness_list) # This outcome is quite long, as a note. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee69b1-fbca-4728-b355-9e25205c9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use .hist to create a histogram based on the matplotlib library. \n",
    "plt.hist(betweenness_list, bins=5) \n",
    "\n",
    "# Setting labels and title\n",
    "plt.xlabel(\"Betweenness Centrality\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.title(\"Distribution of Betweenness Centrality\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b50d6-8737-42bb-812b-3937a65a746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a graph as before. \n",
    "git_network_subset = nx.Graph()\n",
    "git_network_subset.add_edges_from(subset_G)\n",
    "\n",
    "# Getting betweenness centrality \n",
    "betweenness_centrality = nx.betweenness_centrality(G_network_subset)\n",
    "\n",
    "# Plot the graph\n",
    "pos = nx.spring_layout(G_network_subset)\n",
    "nx.draw(G_network_subset, pos, with_labels=True, node_size=700, node_color='skyblue')\n",
    "plt.title('Graph G_network_subset with Betweenness Centrality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6930f5",
   "metadata": {},
   "source": [
    "### Q7. Plot the Matrix, Arc and Circos from the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efa207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency Matrix Plot\n",
    "nv.MatrixPlot(G_sub)\n",
    "plt.title(\"Adjacency Matrix of Subgraph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0092354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arc Diagram\n",
    "plt.title(\"Arc Diagram of Subgraph\")\n",
    "a = nv.ArcPlot(G_sub)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10998a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circos Plot\n",
    "nv.CircosPlot(G_sub)\n",
    "plt.title(\"Circos Plot of Subgraph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d833a-e725-46d1-b7df-4746bc8ac3ff",
   "metadata": {},
   "source": [
    "## Challenge 2: \n",
    "\n",
    "This challenge is about OSMnx. You will explore and analyze a city's street network using the OSMnx Python library.\n",
    "\n",
    "1. Use OSMnx to download the street network of a city of your choice. You can specify the city name, BBox or a Dict.\n",
    "2. Calculate basic statistics for the street network, such as the number of nodes, edges, average node degree, etc.\n",
    "3. Use OSMnx to plot the street network. Customize the plot to make it visually appealing, including node size, edge color. See the potential options here: https://osmnx.readthedocs.io/en/stable/user-reference.html#module-osmnx.plot\n",
    "4. Utilize the routing capabilities of OSMnx to find the shortest path between two points in the street network. Plot the route on top of the street network.\n",
    "5. Calculate the centrality measures (e.g., degree centrality and betweenness_centrality) for nodes in the street network.\n",
    "6. Create the figure-groud from the selected city\n",
    "7. Create interactive maps to plot nodes, edges, nodes+edges and one of the centrality measures.\n",
    "8. Export the street network to a GeoPackage (.gpkg) file. Ensure that the exported file contains both node and edge attributes. Demonstrate that the new GeoPackage can be used and read in Python using any of the libraries we have seen in the class to create a simple and interactive map.\n",
    "9. Finally, use OSMnx to extract other urban elements (e.g., buildings, parks) and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36bccc-62f8-4ca5-a8aa-18a2157a0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9fe65",
   "metadata": {},
   "source": [
    "### Q1. Use OSMnx to download the street network of a city of your choice. You can specify the city name, BBox or a Dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fae25-3716-4a10-85ba-31d5c66edcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Birmingham city centre (Birmingham New Street Station)\n",
    "birmingham_centre = (52.4778, -1.8984)\n",
    "two_miles = 3218.68  # meters\n",
    "\n",
    "# Retrieve the street network for driving routes within one mile\n",
    "G_birmingham = ox.graph_from_point(birmingham_centre, dist=two_miles, network_type=\"drive\")\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = ox.plot_graph(G_birmingham, node_size=0, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to a gdf\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Using graph to gdfs\n",
    "gdf_nodes, gdf_edges = ox.graph_to_gdfs(G_birmingham)\n",
    "# Now there is a geodataframe (G_dub) that has the edges and the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f19a66",
   "metadata": {},
   "source": [
    "### Q2. Calculate basic statistics for the street network, such as the number of nodes, edges, average node degree, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the area in meters of the map \n",
    "warnings.filterwarnings('ignore')\n",
    "G_birmingham_proj = ox.project_graph(G_birmingham)\n",
    "nodes_proj = ox.graph_to_gdfs(G_birmingham_proj, edges=False)\n",
    "graph_area_m = nodes_proj.unary_union.convex_hull.area\n",
    "graph_area_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the map \n",
    "ox.basic_stats(G_birmingham_proj, area=graph_area_m, clean_int_tol=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441426c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I can use .edges/.nodes and len() to print the number of nodes and edges; this is already included above with n and m, but this will make it extra clear. \n",
    "print(len(list(G_birmingham.edges)))\n",
    "print(len(list(G_birmingham.nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1583de6",
   "metadata": {},
   "source": [
    "### Q3. Use OSMnx to plot the street network. Customize the plot to make it visually appealing, including node size, edge color. See the potential options here: https://osmnx.readthedocs.io/en/stable/user-reference.html#module-osmnx.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing the plot using matplotlib\n",
    "fig, ax = ox.plot_graph(G_birmingham,\n",
    "                         figsize=(10, 10),  # Adjusting figure size \n",
    "                         node_size=2,  # Increased the node size to 2 \n",
    "                         edge_color=\"red\",  # Made all the edges red for contrast\n",
    "                         edge_linewidth=0.5,  # Made all the edges 0.5 thickness \n",
    "                         show=False)  # I'd like to add a title later. \n",
    "\n",
    "plt.title(\"Street Network around Birmingham New Street Station Within Two Miles\")  # Adding a title\n",
    "\n",
    "plt.show()  # Displaying the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d888b",
   "metadata": {},
   "source": [
    "### Q4. Utilize the routing capabilities of OSMnx to find the shortest path between two points in the street network. Plot the route on top of the street network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using add_edge_speeds and travel times\n",
    "G_birmingham = ox.speed.add_edge_speeds(G_birmingham)\n",
    "G_birmingham = ox.speed.add_edge_travel_times(G_birmingham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the nearest network nodes to two lat/lng points with the distance module\n",
    "orig = ox.distance.nearest_nodes(G_birmingham, X=-1.8984, Y=52.4778)  # Birmingham New Street Station\n",
    "dest = ox.distance.nearest_nodes(G_birmingham, X=-1.8881, Y=52.4862)  # Aston University\n",
    "\n",
    "print(f\"Origin Node: {orig}, Destination Node: {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa13eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the shortest path between nodes, minimizing travel time, then plotting it\n",
    "route = ox.shortest_path(G_birmingham, orig, dest, weight=\"travel_time\")\n",
    "fig, ax = ox.plot_graph_route(G_birmingham, route, node_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long is our route in meters?\n",
    "edge_lengths = ox.utils_graph.route_to_gdf(G_birmingham, route)[\"length\"]\n",
    "round(sum(edge_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how far is it between these two nodes?\n",
    "# use OSMnx's vectorized great-circle distance (haversine) function\n",
    "orig_x = G_birmingham.nodes[orig][\"x\"]\n",
    "orig_y = G_birmingham.nodes[orig][\"y\"]\n",
    "dest_x = G_birmingham.nodes[dest][\"x\"]\n",
    "dest_y = G_birmingham.nodes[dest][\"y\"]\n",
    "round(ox.distance.great_circle(orig_y, orig_x, dest_y, dest_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040a46a-c2d2-4506-8264-506a52b51be9",
   "metadata": {},
   "source": [
    "COULD DO ELEVATION???? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58452f",
   "metadata": {},
   "source": [
    "### Q5. Calculate the centrality measures (e.g., degree centrality and betweenness_centrality) for nodes in the street network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the graph to line graph so edges become nodes and vice versa\n",
    "edge_centrality = nx.closeness_centrality(nx.line_graph(G_birmingham))\n",
    "nx.set_edge_attributes(G_birmingham, edge_centrality, \"edge_centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colouring the edges in the original graph with closeness centralities from line graph\n",
    "ec = ox.plot.get_edge_colors_by_attr(G_birmingham, \"edge_centrality\", cmap=\"inferno\")\n",
    "fig, ax = ox.plot_graph(G_birmingham, edge_color=ec, edge_linewidth=2, node_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a205b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines here calculate the betweenness centrality for all the nodes in \"G_dub\" and stores them in a dictionary (called betweenness list)\n",
    "betweenness_centrality = nx.betweenness_centrality(G_birmingham)\n",
    "\n",
    "# Extracting betweenness_centrality values as a list\n",
    "betweenness_list = list(betweenness_centrality.values())\n",
    "\n",
    "# Print the betweenness centrality list\n",
    "#print(betweenness_list) # This outcome is quite long, as a note, so feel free to remove the hashtag if you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12febc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the graph to line graph so edges become nodes and vice versa\n",
    "betweenness_centrality = nx.betweenness_centrality(nx.line_graph(G_birmingham))\n",
    "nx.set_edge_attributes(G_birmingham, betweenness_centrality, \"betweenness_centrality\")\n",
    "# coloring the edges in original graph with betweenness centralities from line graph\n",
    "ec = ox.plot.get_edge_colors_by_attr(G_birmingham, \"betweenness_centrality\", cmap=\"inferno\")\n",
    "fig, ax = ox.plot_graph(G_birmingham, edge_color=ec, edge_linewidth=2, node_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773017c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line her|e calculates the degree centrality for all the nodes in \"G_dub\" and stores them in a dictionary (called degree-centrality)\n",
    "degree_centrality = nx.degree_centrality(G_birmingham)\n",
    "\n",
    "# Extracting degree centrality values as a list\n",
    "centrality_list = list(degree_centrality.values())\n",
    "\n",
    "# Print the degree centrality list\n",
    "print(centrality_list) # This outcome is quite long, as a note. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d24949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the graph to line graph so edges become nodes and vice versa\n",
    "degree_centrality = nx.degree_centrality(nx.line_graph(G_birmingham))\n",
    "nx.set_edge_attributes(G_birmingham, degree_centrality, \"degree_centrality\")\n",
    "# coloring the edges in original graph with degree centralities from line graph\n",
    "ec = ox.plot.get_edge_colors_by_attr(G_birmingham, \"degree_centrality\", cmap=\"inferno\")\n",
    "fig, ax = ox.plot_graph(G_birmingham, edge_color=ec, edge_linewidth=2, node_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b90d26",
   "metadata": {},
   "source": [
    "### Q6. Create the figure-groud from the selected city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# configure the inline image display\n",
    "img_folder = \"images\"\n",
    "extension = \"png\"\n",
    "size = 300\n",
    "dpi = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e519d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Birmingham as the new place\n",
    "place = \"birmingham\"\n",
    "point = (52.4778, -1.8984)  # Birmingham New Street Station\n",
    "\n",
    "# Define street widths based on road types\n",
    "street_widths = {\n",
    "    \"footway\": 0.5,\n",
    "    \"steps\": 0.5,\n",
    "    \"pedestrian\": 0.5,\n",
    "    \"path\": 0.5,\n",
    "    \"track\": 0.5,\n",
    "    \"service\": 2,\n",
    "    \"residential\": 3,\n",
    "    \"primary\": 5,\n",
    "    \"motorway\": 6,\n",
    "}\n",
    "\n",
    "# Define output image settings\n",
    "img_folder = \"images\"  # Adjust if needed\n",
    "extension = \"png\"\n",
    "dpi = 300  # High resolution\n",
    "size = 500  # Adjust the display size\n",
    "\n",
    "# File path for saving the image\n",
    "fp = f\"./{img_folder}/{place}.{extension}\"\n",
    "\n",
    "# Generate and save the figure-ground diagram\n",
    "fig, ax = ox.plot_figure_ground(\n",
    "    point=point,\n",
    "    filepath=fp,\n",
    "    network_type=\"all\",\n",
    "    street_widths=street_widths,\n",
    "    dpi=dpi,\n",
    "    save=True,\n",
    "    show=False,\n",
    "    close=True,\n",
    ")\n",
    "\n",
    "# Add the title to the plot\n",
    "ax.set_title(place, fontsize=16, fontweight=\"bold\", pad=15)\n",
    "\n",
    "# Display the image\n",
    "Image(fp, height=size, width=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891d431",
   "metadata": {},
   "source": [
    "### Q7. Create interactive maps to plot nodes, edges, nodes+edges and one of the centrality measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87256c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore graph nodes interactively, with different basemap tiles\n",
    "nodes = ox.graph_to_gdfs(G_birmingham, edges=False)\n",
    "nodes.explore(tiles=\"cartodbpositron\", marker_kwds={\"radius\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d93c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore graph edges interactively, with a simple one-liner\n",
    "ox.graph_to_gdfs(G_birmingham, nodes=False).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db11583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore nodes and edges together in a single map\n",
    "nodes, edges = ox.graph_to_gdfs(G_birmingham)\n",
    "m = edges.explore(color=\"skyblue\", tiles=\"cartodbdarkmatter\")\n",
    "nodes.explore(m=m, color=\"yellow\", marker_kwds={\"radius\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c86b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore graph edges interactively, colored by length\n",
    "edges.explore(tiles=\"cartodbdarkmatter\", column=\"length\", cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73dacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore graph nodes interactively, colored by betweenness centrality\n",
    "nx.set_node_attributes(G_birmingham, nx.betweenness_centrality(G_birmingham, weight=\"length\"), name=\"bc\")\n",
    "nodes = ox.graph_to_gdfs(G_birmingham, edges=False)\n",
    "nodes.explore(tiles=\"cartodbdarkmatter\", column=\"bc\", marker_kwds={\"radius\": 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4699de8",
   "metadata": {},
   "source": [
    "### Q8. Export the street network to a GeoPackage (.gpkg) file. Ensure that the exported file contains both node and edge attributes. Demonstrate that the new GeoPackage can be used and read in Python using any of the libraries we have seen in the class to create a simple and interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5822fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graph to disk as geopackage (for GIS)\n",
    "ox.save_graph_geopackage(G_birmingham, filepath=\"data/birminghamnetwork.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GeoPackage file into a GeoDataFrame\n",
    "gdf_nodes, gdf_edges = gpd.read_file(\"data/birminghamnetwork.gpkg\", layer=\"nodes\"), gpd.read_file(\"data/birminghamnetwork.gpkg\", layer=\"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping both the edges and the nodes into one map using .explore\n",
    "m = gdf_edges.explore(color=\"red\", tiles=\"cartodbdarkmatter\")\n",
    "gdf_nodes.explore(m=m, color=\"skyblue\", marker_kwds={\"radius\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08461f58",
   "metadata": {},
   "source": [
    "### Q9. Finally, use OSMnx to extract other urban elements (e.g., buildings, parks) and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743dd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "# Define the central point of Birmingham (New Street Station) and the radius\n",
    "birmingham_city_centre = (52.4778, -1.8984)\n",
    "two_miles = 3218.68  # meters\n",
    "\n",
    "# Get building footprints within a 2-mile radius\n",
    "building_footprints = ox.geometries_from_point(\n",
    "    birmingham_city_centre,\n",
    "    tags={\"building\": True},\n",
    "    dist=two_miles,\n",
    ")\n",
    "\n",
    "# Get road network (driving)\n",
    "G_birmingham = ox.graph_from_point(\n",
    "    birmingham_city_centre, \n",
    "    dist=two_miles, \n",
    "    network_type=\"drive\"\n",
    ")\n",
    "\n",
    "# Get subway/rail network using a railway filter\n",
    "G_rail = ox.graph_from_point(\n",
    "    birmingham_city_centre, \n",
    "    dist=two_miles, \n",
    "    network_type=\"all\", \n",
    "    custom_filter='[\"railway\"~\"subway|light_rail|tram|rail\"]'\n",
    ")\n",
    "\n",
    "# Create a unified plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot road network in gray\n",
    "ox.plot_graph(G_birmingham, ax=ax, node_size=0, edge_color=\"gray\", edge_linewidth=0.5, show=False)\n",
    "\n",
    "# Plot railway network in red\n",
    "ox.plot_graph(G_rail, ax=ax, node_size=0, edge_color=\"red\", edge_linewidth=1.0, show=False)\n",
    "\n",
    "# Plot building footprints in yellow\n",
    "building_footprints.plot(ax=ax, facecolor=\"yellow\", edgecolor=\"none\", alpha=0.8)\n",
    "\n",
    "# Set title and hide axes\n",
    "ax.set_title(\"Birmingham City Centre: Roads, Railways, and Buildings\", fontsize=16, fontweight=\"bold\", pad=15)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Show final plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the point of interest (Birmingham New Street Station)\n",
    "birmingham_city_centre = (52.4778, -1.8984)\n",
    "two_miles = 3218.68  # meters\n",
    "\n",
    "# Extracting the building footprints within a 2-mile radius of Birmingham New Street\n",
    "building_footprints = ox.geometries_from_point(\n",
    "    birmingham_city_centre,\n",
    "    tags={\"building\": True},\n",
    "    dist=two_miles,\n",
    ")\n",
    "\n",
    "# Plotting the road network graph\n",
    "G_birmingham = ox.graph_from_point(birmingham_city_centre, dist=two_miles, network_type=\"drive\")\n",
    "fig, ax = ox.plot_graph(G_birmingham, node_size=0, show=False)\n",
    "\n",
    "# Plotting the building footprints using .plot\n",
    "building_footprints.plot(ax=ax, facecolor='yellow', alpha=1)  # Highlight buildings in yellow\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# get a subway rail network\n",
    "G = ox.graph_from_place(\n",
    "    \"Edinburgh, UK\",\n",
    "    retain_all=False,\n",
    "    truncate_by_edge=True,\n",
    "    simplify=True,\n",
    "    custom_filter='[\"railway\"]',\n",
    ")\n",
    "\n",
    "fig, ax = ox.plot_graph(G, node_size=0, edge_color=\"w\", edge_linewidth=0.2)\n",
    "\n",
    "# get all building footprints in some neighborhood\n",
    "place = \"Leith, Edinburgh, UK\"\n",
    "tags = {\"building\": True}\n",
    "gdf = ox.features_from_place(place, tags)\n",
    "gdf.shape\n",
    "\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "fig, ax = ox.plot_footprints(gdf, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfc008-d07d-451f-86e8-e4edb053599c",
   "metadata": {},
   "source": [
    "# Lab No 6: Geodemographics (1 Challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeec99d-a5cc-4e2e-b6e6-e10e38fc51b9",
   "metadata": {},
   "source": [
    "## Challenge 1: Geodemographic Classification\n",
    "\n",
    "In this challenge, you will replicate the process of creating a geodemographic classification using the k-means clustering algorithm. Please select any city in the UK except London, Liverpool, or Glasgow. The main goal is to generate a meaningful and informative classification that captures the diversity of areas in your dataset using the census data ( For England, you can try to use the 2021 or 2011 census, and for Scotland, you need to use the 2011 census data) \n",
    "\n",
    "1. Define the main goal for the geodemographic classification (marketing, retail and service planning). \n",
    "2. Look for census data from the selected city for which you would like to generate the geodemographic classification.\n",
    "3. The census data at the Output Area OA level. Select multiple topics of at least four topics (socio-demographics, economics, health, and so on). Describe your topic selection accordingly based on the goal of your geodemographic classification. For example, if your geodemographics are related to marketing, Economic variables might be the appropriate selection. \n",
    "4. Identify the variables that will be crucial for effectively segmenting neighbourhoods. Evaluate how this choice may impact the classification results, including a DEA analysis.\n",
    "5. Prepare, adjust or clean the dataset addressing any missing values or outliers that could distort the clustering results.\n",
    "6. Include standardisation between areas and variables. Make an appropriate analysis and adjust the variable selection accordingly for any multicollinearity.\n",
    "7. Utilize the k-means clustering algorithm to create a classification based on the selected variables.\n",
    "8. Define the optimum number of clusters (i.e., using the Elblow method). Experiment with different values of k.\n",
    "9. Evaluate your cluster groups (e.g., using PCA) and interpret your cluster centres. Describe your results and repeat the process to adjust the variable selection and cluster groups to provide more meaningful results for your geodemographic goal. Interpret the characteristics of each cluster. What demographic patterns or similarities are prevalent within each group?\n",
    "10. Map the final cluster groups\n",
    "11. Finish the analysis by naming the final clusters and plotting a final map that includes the census values and the provided names.\n",
    "12. Finally, acknowledge the subjective nature of classification and make analytical decisions to produce an optimum classification for your specific purpose. Reflect on the challenges and insights gained during the classification process. Ensure you document your analytical decisions and the rationale behind any important decision. Once your geodemographics are constructed, describe the potential use cases for the geodemographic classification you have built based on your initial goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19a368",
   "metadata": {},
   "source": [
    "### Q1. Define the main goal for the geodemographic classification (marketing, retail and service planning). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dcd88",
   "metadata": {},
   "source": [
    "The objective of this geodemographic classification is to assist in **retail planning** in Birmingham. This classification will help businesses understand the socio-economic and demographic characteristics of different neighbourhoods to make data-driven decisions on store locations, targeted marketing, and service provision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5d84e",
   "metadata": {},
   "source": [
    "### Q2. Look for census data from the selected city for which you would like to generate the geodemographic classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b165f",
   "metadata": {},
   "source": [
    "We will use **England's 2021 Census Data** at the **Output Area (OA) level** for Birmingham. The dataset includes various demographic and socio-economic indicators essential for segmenting urban areas into meaningful clusters.\n",
    "\n",
    "I found bulk census data for Output Areas in England based on the 2021 census from [NOMIS](https://www.nomisweb.co.uk/sources/census_2021_bulk). I then downloaded the data for the variables below as csv files. \n",
    "\n",
    "Additionally, I found the output area classification shapefile for the entire UK from the following source: https://data.cdrc.ac.uk/dataset/output-area-classification-2011#data-and-resources. I also used local planning authority boundaries from a previously used shapefile from Lab No 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbc1ef",
   "metadata": {},
   "source": [
    "### Q3. The census data at the Output Area OA level. Select multiple topics of at least four topics (socio-demographics, economics, health, and so on). Describe your topic selection accordingly based on the goal of your geodemographic classification. For example, if your geodemographics are related to marketing, Economic variables might be the appropriate selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bb002",
   "metadata": {},
   "source": [
    "To ensure a comprehensive geodemographic classification, we will incorporate variables from the following four broad topics:\n",
    "   - **Socio-demographics**: Age distribution, household composition, ethnicity, migration patterns\n",
    "   - **Economics**: Employment status, occupation types, income levels\n",
    "   - **Health**: General health status, disability prevalence\n",
    "   - **Housing & Living Conditions**: Housing tenure, overcrowding, access to essential services\n",
    "\n",
    "These topics align with our objective as they influence purchasing power, service needs, and consumer behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755beef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_directory = \"/Users/elenajarrett/Library/CloudStorage/OneDrive-Personal/Year 4/GG4257/Data2/Data/data6/census_raw_data/\"\n",
    "\n",
    "# We need a list of all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(csv_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "# An empty DataFrame to store the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    csv_path = os.path.join(csv_directory, csv_file) # We create a consistent path\n",
    "    df_csv = pd.read_csv(csv_path, low_memory=False) #read each file\n",
    "    # Concatenate/Merge all columns, there is a pitfall here, you will get a duplicate oa_code from all csv files.\n",
    "    merged_data = pd.concat([merged_data, df_csv], axis=1)\n",
    "\n",
    "# Save the merged dataset. You might want to do some pre-processing.\n",
    "merged_data.to_csv(\"/Users/elenajarrett/Library/CloudStorage/OneDrive-Personal/Year 4/GG4257/Data2/Data/data6/census_raw_data/merged_census_data.csv\", index=False)\n",
    "# Be aware of the mixted dtype you are importing we unfortunatly have to deal with that later.\n",
    "# eventually you can avoid this to define the dtype on import method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the bounds of Birmingham \n",
    "import geopandas as gpd\n",
    "UK_LPA = \"/Users/elenajarrett/Library/CloudStorage/OneDrive-Personal/Year 4/GG4257/Data2/Data/data6/LAP_2021/LPA_MAY_2021_UK_BUC_V2.shp\" # This is the shapefile for all the local planning authorities in the UK\n",
    "gdf_UK_LPA = gpd.read_file(UK_LPA) # Reading it into a geodataframe from the shapefile\n",
    "birmingham_lpa_gdf = gdf_UK_LPA[gdf_UK_LPA.LPA21NM == \"Birmingham LPA\"] # This filters the LPAs by name, specifically Birmingham. \n",
    "\n",
    "# Defining the output areas in UK \n",
    "oa_shapefile = gpd.read_file(\"/Users/elenajarrett/Library/CloudStorage/OneDrive-Personal/Year 4/GG4257/Data2/Data/data6/2011_OAC.shp\")\n",
    "\n",
    "# Clipping the OAs to Birmingham city using .clip \n",
    "birmingham_oa = gpd.clip(oa_shapefile, birmingham_lpa_gdf)\n",
    "# Saving it to a file\n",
    "birmingham_oa.to_file(\"/Users/elenajarrett/Library/CloudStorage/OneDrive-Personal/Year 4/GG4257/Data2/Data/data6/birmingham_oa.shp\", driver='ESRI Shapefile') # Saving it as a shapefile to the folder, which is the file I provided for you. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cc9aa",
   "metadata": {},
   "source": [
    "For the geodemographic classification, we will use the following 2021 Census datasets:\n",
    "   - **TS001**: Number of usual residents in households and communal establishments\n",
    "   - **TS006**: Population density\n",
    "   - **TS008**: Sex \n",
    "   - **TS007A**: Age by five-year age bands\n",
    "   - **TS011**: Households by deprivation dimensions\n",
    "   - **TS021**: Ethnic group\n",
    "   - **TS029**: Proficiency in English\n",
    "   - **TS037**: General health\n",
    "   - **TS038**: Disability\n",
    "   - **TS041**: Number of households\n",
    "   - **TS054**: Tenure\n",
    "   - **TS058**: Distance travelled to work\n",
    "   - **TS063**: Occupation\n",
    "   - **TS066**: Economic activity status\n",
    "   - **TS067**: Highest level of qualification\n",
    "   - **TS068**: Schoolchildren and full-time students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b93453",
   "metadata": {},
   "outputs": [],
   "source": [
    "birmingham_oa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e83b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merged_data.columns)\n",
    "#This is just a small sample of the number of potential variables you have available in the census\n",
    "# Using some ML techniques, you could include hundreds of these to make your geodemographics\n",
    "# more meaningful. For this academic exercise, only 386 variables will be loaded,\n",
    "# and then filtered to be included in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75921381",
   "metadata": {},
   "source": [
    "### Q4. Identify the variables that will be crucial for effectively segmenting neighbourhoods. Evaluate how this choice may impact the classification results, including a DEA analysis.\n",
    "\n",
    "Key variables chosen for clustering:\n",
    "   - **Socio-demographics**: Percentage of young (0-18), working-age (18-64), and elderly (65+)\n",
    "   - **Economics**: Unemployment rate, percentage of professionals vs. manual laborers\n",
    "   - **Health**: Percentage reporting ‘bad’ or ‘very bad’ health\n",
    "   - **Housing**: Percentage of households in social housing, percentage of owner-occupied homes\n",
    "\n",
    "Impact Analysis:\n",
    "   - A high percentage of young people may indicate demand for educational services and youth-oriented retail.\n",
    "   - High unemployment rates may reflect areas with lower spending power.\n",
    "   - Poor health indicators could highlight areas needing healthcare services and accessible facilities.\n",
    "   - Housing tenure affects stability and service demand—areas with more rented accommodations may have higher population turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6834ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "merged_data['Economically active: Employee: Part-time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae74447",
   "metadata": {},
   "source": [
    "DEA ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "attributes_to_plot = ['30 to 44',\n",
    "                      'Church of Scotland',\n",
    "                      'Gas central heating',\n",
    "                      'Economically active: Unemployed']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, attribute in enumerate(attributes_to_plot, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.violinplot(x=merged_data[attribute])\n",
    "    plt.title(attribute)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea028b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "attributes_to_plot = ['30 to 44',\n",
    "                      'Church of Scotland',\n",
    "                      'Gas central heating',\n",
    "                      'Economically active: Unemployed']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, attribute in enumerate(attributes_to_plot, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.histplot(merged_data[attribute].astype(str), kde=True)\n",
    "    plt.title(attribute)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d46349",
   "metadata": {},
   "source": [
    "### Q5. Prepare, adjust or clean the dataset addressing any missing values or outliers that could distort the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef97b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentages(dataframe, total_columns, value_columns):\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    for total_col, value_col in zip(total_columns, value_columns):\n",
    "        percentage_col_name = f\"{value_col}_percentage\"\n",
    "\n",
    "        if total_col not in dataframe.columns:\n",
    "            raise ValueError(f\"Total column '{total_col}' not found in the DataFrame.\")\n",
    "        # The data have several empty values, so you need to process them, forcing them to be a numeric or NaN value\n",
    "        dataframe[value_col] = pd.to_numeric(dataframe[value_col], errors='coerce')\n",
    "        dataframe[total_col] = pd.to_numeric(dataframe[total_col], errors='coerce')\n",
    "        \n",
    "        result_df[percentage_col_name] = (dataframe[value_col] / dataframe[total_col]) * 100\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# List of the corresponding totals.\n",
    "total_cols = ['All people aged 16 and over',\n",
    "              'All people',\n",
    "              'All people.1','All people.1',\n",
    "              'All people aged 16 and over.1', 'All people aged 16 and over.1',\n",
    "              'All households','All households',\n",
    "              'All occupied household spaces', 'All occupied household spaces',\n",
    "              'All people.2','All people.2',\n",
    "              'All people aged 16 to 74', 'All people aged 16 to 74',\n",
    "              'All people.3', 'All people.3',\n",
    "              'All people aged 16 to 74 in employment','All people aged 16 to 74 in employment',\n",
    "              'All people aged 16 to 74 in employment' ]\n",
    "# List of the corresponding values. \n",
    "value_cols = ['All people aged 16 and over: Level 4 and above',\n",
    "              '30 to 44',\n",
    "              'Church of Scotland',\n",
    "              'No religion',\n",
    "              'Single (never married or never registered a same-sex civil partnership)',\n",
    "              'Married',\n",
    "              'One family only: Married or same-sex civil partnership couple: With dependent children',\n",
    "              'One person household: Aged under 65',\n",
    "              'Gas central heating',\n",
    "              'Electric (including storage heaters) central heating',\n",
    "              'Males',\n",
    "              'Females',\n",
    "              'Economically active: Employee: Full-time',\n",
    "              'Economically active: Unemployed',\n",
    "              'Good health',\n",
    "              'Bad health',\n",
    "              'F. Construction', 'J. Information and communication', 'P. Education']\n",
    "\n",
    "# Later we migth need to rename those columns., for now is ok...\n",
    "result_dataframe = calculate_percentages(merged_data, total_cols, value_cols)\n",
    "\n",
    "# Likely you could also do all this in Excel :D\n",
    "# but what's important is you are aware of the correct total as dominator, census data can be very tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the resulting tables.\n",
    "concatenated_df = pd.concat([merged_data, result_dataframe], axis=1, ignore_index=False)\n",
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24061c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72218d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(concatenated_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the attributes we need, we dont need the total for now.\n",
    "keep_cols= [\n",
    "    'code',\n",
    "    'Popcount',\n",
    "    'HHcount',\n",
    "    'DataZone',\n",
    "    'geometry',\n",
    "    'All people aged 16 and over: Level 4 and above_percentage',\n",
    "     '30 to 44_percentage',\n",
    "     'Church of Scotland_percentage',\n",
    "     'No religion_percentage',\n",
    "     'Single (never married or never registered a same-sex civil partnership)_percentage',\n",
    "     'Married_percentage',\n",
    "     'One family only: Married or same-sex civil partnership couple: With dependent children_percentage',\n",
    "     'One person household: Aged under 65_percentage',\n",
    "     'Gas central heating_percentage',\n",
    "     'Electric (including storage heaters) central heating_percentage',\n",
    "     'Males_percentage',\n",
    "     'Females_percentage',\n",
    "     'Economically active: Employee: Full-time_percentage',\n",
    "     'Economically active: Unemployed_percentage',\n",
    "     'Good health_percentage',\n",
    "     'Bad health_percentage',\n",
    "     'F. Construction_percentage',\n",
    "     'J. Information and communication_percentage',\n",
    "     'P. Education_percentage'\n",
    "]\n",
    "\n",
    "glasglow_census_data = concatenated_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec707fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasglow_census_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more easy manipulation we define short column names\n",
    "short_column_names = {\n",
    "    'All people aged 16 and over: Level 4 and above_percentage': 'Edu_Level4',\n",
    "    '30 to 44_percentage': '30to44',\n",
    "    'Church of Scotland_percentage': 'Ch_of_Scot',\n",
    "    'No religion_percentage': 'No_Religion',\n",
    "    'Single (never married or never registered a same-sex civil partnership)_percentage': 'Single',\n",
    "    'Married_percentage': 'Married',\n",
    "    'One family only: Married or same-sex civil partnership couple: With dependent children_percentage': 'Family_With_Children',\n",
    "    'One person household: Aged under 65_percentage': 'Family_1Person',\n",
    "    'Gas central heating_percentage': 'Gas_Heating',\n",
    "    'Electric (including storage heaters) central heating_percentage': 'Electric_Heating',\n",
    "    'Males_percentage': 'Males',\n",
    "    'Females_percentage': 'Females',\n",
    "    'Economically active: Employee: Full-time_percentage': 'Emp_FullTime',\n",
    "    'Economically active: Unemployed_percentage': 'Unemployed',\n",
    "    'Good health_percentage': 'Good_Health',\n",
    "    'Bad health_percentage': 'Bad_Health',\n",
    "    'F. Construction_percentage': 'Construction',\n",
    "    'J. Information and communication_percentage': 'Info_Comm',\n",
    "    'P. Education_percentage': 'Education'\n",
    "}\n",
    "\n",
    "glasglow_census_data = glasglow_census_data.rename(columns=short_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c34897",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasglow_census_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca840d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glasglow_census_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277a10f",
   "metadata": {},
   "source": [
    "### Q6. Include standardisation between areas and variables. Make an appropriate analysis and adjust the variable selection accordingly for any multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13833a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-score for each column, but we need to initially filter only the float attributes.\n",
    "# bcs you can't calculate that for the OA code.. :P\n",
    "\n",
    "numeric_columns = glasglow_census_data.select_dtypes(include='float64')\n",
    "z_score_df = (numeric_columns - numeric_columns.mean()) / numeric_columns.std(ddof=0)\n",
    "z_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = z_score_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(z_score_df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ea9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7 # We can adapt this based on waht we have in our data. Recall the subjetivity issue?\n",
    "# So if we include .8 then we wont be able to reduce any variables., so I took the threshold to 70%\n",
    "\n",
    "highly_correlated = (corr.abs() > threshold) & (corr.abs() < 1.0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(highly_correlated, cmap='coolwarm', cbar=False, annot=True)\n",
    "\n",
    "plt.title('Highly Correlated Variables')\n",
    "plt.show()\n",
    "\n",
    "# The plot will represent  BINARY table 0 for false( out of the threshold) and 1 for above the threshold.\n",
    "# but I also coloured so it is easier to use and cute :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b08884",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cff8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.drop(['Family_With_Children', 'Gas_Heating', 'Single'], axis=1, inplace=True)\n",
    "z_score_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_2 = z_score_df.corr()\n",
    "corr_2.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "highly_correlated_2 = (corr_2.abs() > threshold) & (corr_2.abs() < 1.0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(highly_correlated_2, cmap='coolwarm', cbar=False, annot=True)\n",
    "\n",
    "plt.title('New Highly Correlated Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_nan = z_score_df.isna().any().any()\n",
    "\n",
    "if contains_nan:\n",
    "    print(\"Oh no :( the DataFrame contains NaN values.\")\n",
    "else:\n",
    "    print(\"Wahoo! the dataFrame does not contain NaN values. I'm the best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.fillna(z_score_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21681111",
   "metadata": {},
   "source": [
    "### Q7. Utilize the k-means clustering algorithm to create a classification based on the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "?KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92064f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "# KMeans with 10 clusters\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(z_score_df)\n",
    "labels = kmeans.predict(z_score_df)\n",
    "cluster_centres = kmeans.cluster_centers_\n",
    "z_score_df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea12a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels)\n",
    "# Ok, we clustered the data, but we subjectively defined the number of clusters, \n",
    "# and as you can see is one of the key parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff658f",
   "metadata": {},
   "source": [
    "### Q8. Define the optimum number of clusters (i.e., using the Elblow method). Experiment with different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "\n",
    "K_range = range(1,15)\n",
    "\n",
    "for k in K_range:\n",
    " km = KMeans(n_clusters=k)\n",
    " km = km.fit(z_score_df)\n",
    " Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K_range, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ae2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def elbow(dataframe, n):\n",
    "    kMeansVar = [KMeans(n_clusters=k).fit(dataframe.values) for k in range(1, n)] #making use of list comprehensions.\n",
    "    centroids = [X.cluster_centers_ for X in kMeansVar]\n",
    "    k_euclid = [cdist(dataframe.values, cent) for cent in centroids]\n",
    "    dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    tss = sum(pdist(dataframe.values)**2)/dataframe.values.shape[0]\n",
    "    bss = tss - wcss\n",
    "    plt.plot(bss)\n",
    "    plt.show()\n",
    " \n",
    "elbow(z_score_df,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e268a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans with 6 clusters, after the validation with the Elbow method.\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "kmeans.fit(z_score_df)\n",
    "labels = kmeans.predict(z_score_df)\n",
    "cluster_centres = kmeans.cluster_centers_\n",
    "\n",
    "z_score_df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c06b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8de3fb",
   "metadata": {},
   "source": [
    "### Q9. Evaluate your cluster groups (e.g., using PCA) and interpret your cluster centres. Describe your results and repeat the process to adjust the variable selection and cluster groups to provide more meaningful results for your geodemographic goal. Interpret the characteristics of each cluster. What demographic patterns or similarities are prevalent within each group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e67914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on the example provided here: \n",
    "# https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusters = kmeans.fit_predict(z_score_df)\n",
    "\n",
    "z_score_df['Cluster'] = clusters\n",
    "\n",
    "scaler = StandardScaler()\n",
    "stand_data_scaled = scaler.fit_transform(z_score_df)\n",
    "\n",
    "# PCA analysys.\n",
    "pca = PCA(n_components=2).fit(stand_data_scaled)\n",
    "pca_result = pca.transform(stand_data_scaled)\n",
    "\n",
    "#Percentage of variance explained by each of the selected components.\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a scatter plot\n",
    "fig = px.scatter(x=pca_result[:, 0], y=pca_result[:, 1], color=clusters,\n",
    "                 labels={'color': 'Cluster'},\n",
    "                 #title='Cluster Plot against 1st 2 Principal Components',\n",
    "                 opacity=0.7,\n",
    "                 width=800, \n",
    "                 height=800)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "print(f\"These two components explain {(variance_ratio.sum()*100):.2f}% of the point variability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a static figure with the point variability included in the x/y-axis label.\n",
    "# So we can see what variability is provided by each component.\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusters = kmeans.fit_predict(z_score_df)\n",
    "\n",
    "z_score_df['Cluster'] = clusters\n",
    "\n",
    "# Standardize the data for PCA\n",
    "scaler = StandardScaler()\n",
    "stand_data_scaled = scaler.fit_transform(z_score_df)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2).fit(stand_data_scaled)\n",
    "pca_result = pca.transform(stand_data_scaled)\n",
    "\n",
    "#Percentage of variance explained by each of the selected components.\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=clusters, palette='viridis', s=50, alpha=0.7)\n",
    "plt.title('Cluster Plot against 1st 2 Principal Components')\n",
    "plt.xlabel(f'Principal Component 1 variation: {variance_ratio[0]*100:.2f}%')\n",
    "plt.ylabel(f'Principal Component 2 variation: {variance_ratio[1]*100:.2f}%')\n",
    "plt.legend(title='Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baba461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusters = kmeans.fit_predict(z_score_df)\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=z_score_df.columns)\n",
    "\n",
    "# Create a new DataFrame with cluster assignments and column names\n",
    "#result_df = pd.DataFrame({'Cluster': clusters, 'Column': z_score_df.columns})\n",
    "\n",
    "cluster_centers.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from:\n",
    "# https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_demo.html\n",
    "\n",
    "first_row_centers = cluster_centers.iloc[0, :]\n",
    "\n",
    "# len of features\n",
    "num_features = len(first_row_centers)\n",
    "\n",
    "# polar coordinates\n",
    "theta = np.linspace(0, 2 * np.pi, num_features, endpoint=True)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "ax.plot(theta, first_row_centers, linewidth=1, color='blue', marker='o', label='Centers')\n",
    "# Add an extra red line at the 0.0 value\n",
    "ax.plot(theta, np.zeros_like(first_row_centers), color='red', linestyle='--', label='Avarage')\n",
    "\n",
    "ax.set_xticks(theta)\n",
    "ax.set_xticklabels(cluster_centers.columns, rotation=45, ha='right')\n",
    "\n",
    "plt.show()\n",
    "#Ignore the cluster polar values, and focus in he census variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d872fd",
   "metadata": {},
   "source": [
    "### Q10. Map the final cluster groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(z_score_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dafd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df.drop([\n",
    " 'Edu_Level4',\n",
    " '30to44',\n",
    " 'Ch_of_Scot',\n",
    " 'No_Religion',\n",
    " 'Married',\n",
    " 'Family_1Person',\n",
    " 'Electric_Heating',\n",
    " 'Males',\n",
    " 'Females',\n",
    " 'Emp_FullTime',\n",
    " 'Unemployed',\n",
    " 'Good_Health',\n",
    " 'Bad_Health',\n",
    " 'Construction',\n",
    " 'Info_Comm',\n",
    " 'Education'], axis=1, inplace=True)\n",
    "z_score_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the resulting tables.\n",
    "final_df = pd.concat([glasglow_census_data, z_score_df], axis=1, ignore_index=False)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5afcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.explore(column='Cluster', cmap='Set1', tiles='CartoDB positron')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343b2f0",
   "metadata": {},
   "source": [
    "### Q11. Finish the analysis by naming the final clusters and plotting a final map that includes the census values and the provided names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c83124",
   "metadata": {},
   "source": [
    "### Q12. Finally, acknowledge the subjective nature of classification and make analytical decisions to produce an optimum classification for your specific purpose. Reflect on the challenges and insights gained during the classification process. Ensure you document your analytical decisions and the rationale behind any important decision. Once your geodemographics are constructed, describe the potential use cases for the geodemographic classification you have built based on your initial goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6440b-3a4f-4a99-a807-19a4d1ef1aa6",
   "metadata": {},
   "source": [
    "# Lab No 7: Spatial Microsimulation (3 Challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab613243-ba52-476b-b88c-ffddadbdb7e8",
   "metadata": {},
   "source": [
    "## Challenge 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169811b-991b-4e6d-8a52-57148eec2252",
   "metadata": {},
   "source": [
    "## Challenge 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a71392-c3d1-4a33-8366-f11af8ad5f02",
   "metadata": {},
   "source": [
    "## Challenge 3: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db6c2e",
   "metadata": {},
   "source": [
    "# Final Remarks (limitations, barriers, and any additional comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
